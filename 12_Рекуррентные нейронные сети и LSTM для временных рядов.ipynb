{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Рекуррентные нейронные сети и LSTM для временных рядов\n",
    "\n",
    "## Введение\n",
    "\n",
    "Рекуррентные нейронные сети (RNN) и их улучшенные варианты, такие как LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit), представляют собой мощный инструмент для моделирования временных рядов. В отличие от классических статистических методов, глубокое обучение способно автоматически извлекать сложные нелинейные зависимости из данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Теоретические основы\n",
    "\n",
    "### 1.1 Рекуррентные нейронные сети (RNN)\n",
    "\n",
    "RNN - это класс нейронных сетей, которые имеют циклические связи, позволяющие информации сохраняться между шагами обработки последовательности.\n",
    "\n",
    "**Основное уравнение RNN:**\n",
    "\n",
    "$$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "\n",
    "$$y_t = W_{hy}h_t + b_y$$\n",
    "\n",
    "где:\n",
    "- $h_t$ - скрытое состояние в момент времени $t$\n",
    "- $x_t$ - входной вектор в момент времени $t$\n",
    "- $y_t$ - выходной вектор в момент времени $t$\n",
    "- $W$ - матрицы весов\n",
    "- $b$ - векторы смещений\n",
    "\n",
    "**Проблемы простых RNN:**\n",
    "- Проблема затухающего градиента (vanishing gradient)\n",
    "- Проблема взрывающегося градиента (exploding gradient)\n",
    "- Сложность в обучении долгосрочных зависимостей\n",
    "\n",
    "### 1.2 LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM были разработаны для решения проблемы долгосрочных зависимостей. Они используют специальную архитектуру с \"воротами\" (gates), которые регулируют поток информации.\n",
    "\n",
    "**Основные компоненты LSTM:**\n",
    "\n",
    "1. **Forget Gate (Вентиль забывания):** решает, какую информацию удалить из состояния ячейки\n",
    "   $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **Input Gate (Входной вентиль):** решает, какую новую информацию добавить\n",
    "   $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "   $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3. **Cell State Update (Обновление состояния ячейки):**\n",
    "   $$C_t = f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t$$\n",
    "\n",
    "4. **Output Gate (Выходной вентиль):**\n",
    "   $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "   $$h_t = o_t \\ast \\tanh(C_t)$$\n",
    "\n",
    "где $\\sigma$ - сигмоидная функция, $\\ast$ - поэлементное умножение.\n",
    "\n",
    "### 1.3 GRU (Gated Recurrent Unit)\n",
    "\n",
    "GRU - упрощенная версия LSTM с меньшим числом параметров:\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$ (update gate)\n",
    "\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$ (reset gate)\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(W \\cdot [r_t \\ast h_{t-1}, x_t])$$\n",
    "\n",
    "$$h_t = (1 - z_t) \\ast h_{t-1} + z_t \\ast \\tilde{h}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Практическая реализация\n",
    "\n",
    "### 2.1 Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Для работы с LSTM понадобится TensorFlow/Keras или PyTorch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Настройка визуализации\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация синтетических данных временного ряда\n",
    "np.random.seed(42)\n",
    "time = np.arange(0, 1000)\n",
    "trend = 0.02 * time\n",
    "seasonality = 10 * np.sin(2 * np.pi * time / 50)\n",
    "noise = np.random.randn(len(time)) * 2\n",
    "ts_data = trend + seasonality + noise + 50\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame({'time': time, 'value': ts_data})\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df['time'], df['value'], label='Временной ряд')\n",
    "plt.xlabel('Время')\n",
    "plt.ylabel('Значение')\n",
    "plt.title('Исходный временной ряд')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Размер данных: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Подготовка данных для LSTM\n",
    "\n",
    "Для обучения LSTM необходимо преобразовать временной ряд в формат (samples, timesteps, features). Создадим функцию для создания последовательностей с окном (window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size, horizon=1):\n",
    "    \"\"\"\n",
    "    Создание последовательностей для обучения LSTM\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Исходный временной ряд\n",
    "    window_size : int\n",
    "        Размер окна (количество предыдущих наблюдений)\n",
    "    horizon : int\n",
    "        Горизонт прогнозирования (количество шагов вперед)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X, y : tuple of arrays\n",
    "        Входные последовательности и целевые значения\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size:i + window_size + horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Нормализация данных (важно для нейронных сетей)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[['value']])\n",
    "\n",
    "# Параметры\n",
    "window_size = 50  # Используем 50 предыдущих наблюдений\n",
    "horizon = 1       # Прогнозируем 1 шаг вперед\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size - window_size:]  # включаем окно для первого предсказания\n",
    "\n",
    "# Создание последовательностей\n",
    "X_train, y_train = create_sequences(train_data, window_size, horizon)\n",
    "X_test, y_test = create_sequences(test_data, window_size, horizon)\n",
    "\n",
    "# Reshape для LSTM: (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Построение модели LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание архитектуры LSTM модели\n",
    "def build_lstm_model(input_shape, units=[50, 50], dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Построение LSTM модели\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Размерность входа (timesteps, features)\n",
    "    units : list\n",
    "        Список размерностей LSTM слоев\n",
    "    dropout_rate : float\n",
    "        Коэффициент dropout для регуляризации\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Первый LSTM слой\n",
    "    model.add(LSTM(units=units[0], return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Второй LSTM слой\n",
    "    model.add(LSTM(units=units[1], return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Выходной слой\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Компиляция\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Построение модели\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape, units=[64, 32], dropout_rate=0.2)\n",
    "\n",
    "# Вывод архитектуры\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка колбэков\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация процесса обучения\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Ошибка обучения и валидации')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Средняя абсолютная ошибка')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Прогнозирование и оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение предсказаний\n",
    "train_predictions = lstm_model.predict(X_train)\n",
    "test_predictions = lstm_model.predict(X_test)\n",
    "\n",
    "# Обратная трансформация (денормализация)\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Вычисление метрик\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual, train_predictions))\n",
    "train_mae = mean_absolute_error(y_train_actual, train_predictions)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual, test_predictions))\n",
    "test_mae = mean_absolute_error(y_test_actual, test_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Метрики качества модели LSTM\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Train MAE: {train_mae:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация предсказаний\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Индексы для построения\n",
    "train_indices = range(window_size, window_size + len(train_predictions))\n",
    "test_indices = range(train_size, train_size + len(test_predictions))\n",
    "\n",
    "# Исходные данные\n",
    "plt.plot(df['time'], df['value'], label='Исходные данные', alpha=0.6, linewidth=1)\n",
    "\n",
    "# Предсказания на обучающей выборке\n",
    "plt.plot(train_indices, train_predictions, label='Предсказания (train)', alpha=0.8, linewidth=2)\n",
    "\n",
    "# Предсказания на тестовой выборке\n",
    "plt.plot(test_indices, test_predictions, label='Предсказания (test)', alpha=0.8, linewidth=2, color='red')\n",
    "\n",
    "# Вертикальная линия разделения\n",
    "plt.axvline(x=train_size, color='black', linestyle='--', label='Train/Test split')\n",
    "\n",
    "plt.xlabel('Время')\n",
    "plt.ylabel('Значение')\n",
    "plt.title('Прогнозирование временного ряда с помощью LSTM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Сравнение с GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение GRU модели для сравнения\n",
    "def build_gru_model(input_shape, units=[50, 50], dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=units[0], return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(GRU(units=units[1], return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Обучение GRU\n",
    "gru_model = build_gru_model(input_shape, units=[64, 32], dropout_rate=0.2)\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Предсказания GRU\n",
    "gru_test_predictions = gru_model.predict(X_test)\n",
    "gru_test_predictions = scaler.inverse_transform(gru_test_predictions)\n",
    "\n",
    "# Метрики GRU\n",
    "gru_test_rmse = np.sqrt(mean_squared_error(y_test_actual, gru_test_predictions))\n",
    "gru_test_mae = mean_absolute_error(y_test_actual, gru_test_predictions)\n",
    "\n",
    "print(\"\\nСравнение LSTM и GRU:\")\n",
    "print(f\"LSTM Test RMSE: {test_rmse:.4f} | GRU Test RMSE: {gru_test_rmse:.4f}\")\n",
    "print(f\"LSTM Test MAE: {test_mae:.4f} | GRU Test MAE: {gru_test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Многошаговое прогнозирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_forecast(model, initial_sequence, scaler, n_steps):\n",
    "    \"\"\"\n",
    "    Многошаговое прогнозирование\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Обученная модель\n",
    "    initial_sequence : array\n",
    "        Начальная последовательность\n",
    "    scaler : MinMaxScaler\n",
    "        Скейлер для денормализации\n",
    "    n_steps : int\n",
    "        Количество шагов прогноза\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    current_sequence = initial_sequence.copy()\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Предсказание следующего значения\n",
    "        pred = model.predict(current_sequence.reshape(1, window_size, 1), verbose=0)\n",
    "        forecast.append(pred[0, 0])\n",
    "        \n",
    "        # Обновление последовательности (скользящее окно)\n",
    "        current_sequence = np.append(current_sequence[1:], pred[0, 0])\n",
    "    \n",
    "    # Денормализация\n",
    "    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
    "    return forecast\n",
    "\n",
    "# Многошаговый прогноз на 100 шагов вперед\n",
    "n_forecast_steps = 100\n",
    "last_sequence = scaled_data[train_size - window_size:train_size].flatten()\n",
    "multi_step_pred = multi_step_forecast(lstm_model, last_sequence, scaler, n_forecast_steps)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['time'][:train_size], df['value'][:train_size], label='Обучающая выборка')\n",
    "plt.plot(df['time'][train_size:train_size + n_forecast_steps], \n",
    "         df['value'][train_size:train_size + n_forecast_steps], \n",
    "         label='Фактические значения', alpha=0.7)\n",
    "plt.plot(range(train_size, train_size + n_forecast_steps), \n",
    "         multi_step_pred, \n",
    "         label='Многошаговый прогноз', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=train_size, color='black', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Время')\n",
    "plt.ylabel('Значение')\n",
    "plt.title(f'Многошаговое прогнозирование на {n_forecast_steps} шагов вперед')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Продвинутые техники\n",
    "\n",
    "### 3.1 Bidirectional LSTM\n",
    "\n",
    "Двунаправленные LSTM обрабатывают последовательность в обоих направлениях, что может улучшить качество прогнозов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Bidirectional LSTM модель\n",
    "bi_lstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "bi_lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "bi_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encoder-Decoder архитектура для sequence-to-sequence\n",
    "\n",
    "Для многошагового прогнозирования более эффективна архитектура encoder-decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RepeatVector, TimeDistributed\n",
    "\n",
    "# Параметры для seq2seq\n",
    "n_features = 1\n",
    "forecast_horizon = 10  # Прогнозируем 10 шагов вперед\n",
    "\n",
    "# Подготовка данных для seq2seq\n",
    "X_seq2seq_train, y_seq2seq_train = create_sequences(train_data, window_size, forecast_horizon)\n",
    "X_seq2seq_test, y_seq2seq_test = create_sequences(test_data, window_size, forecast_horizon)\n",
    "\n",
    "X_seq2seq_train = X_seq2seq_train.reshape((X_seq2seq_train.shape[0], window_size, n_features))\n",
    "X_seq2seq_test = X_seq2seq_test.reshape((X_seq2seq_test.shape[0], window_size, n_features))\n",
    "\n",
    "# Encoder-Decoder модель\n",
    "encoder_decoder_model = Sequential([\n",
    "    # Encoder\n",
    "    LSTM(64, activation='relu', input_shape=(window_size, n_features)),\n",
    "    RepeatVector(forecast_horizon),\n",
    "    \n",
    "    # Decoder\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(1))\n",
    "])\n",
    "\n",
    "encoder_decoder_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "encoder_decoder_model.summary()\n",
    "\n",
    "print(f\"\\nФорма входных данных: {X_seq2seq_train.shape}\")\n",
    "print(f\"Форма целевых данных: {y_seq2seq_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Attention механизм\n",
    "\n",
    "Механизм внимания позволяет модели фокусироваться на наиболее релевантных частях входной последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Multiply, Permute, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', \n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Вычисление весов внимания\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# LSTM модель с Attention\n",
    "lstm_attention_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    AttentionLayer(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_attention_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print(\"LSTM модель с механизмом Attention создана\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Практические рекомендации\n",
    "\n",
    "### 4.1 Выбор гиперпараметров\n",
    "\n",
    "**Размер окна (window_size):**\n",
    "- Для данных с сезонностью: минимум один полный сезон\n",
    "- Для данных с трендом: 50-200 наблюдений\n",
    "- Экспериментируйте с разными значениями\n",
    "\n",
    "**Количество LSTM слоев и нейронов:**\n",
    "- Начните с 1-2 слоев по 32-64 нейрона\n",
    "- Увеличивайте при наличии сложных паттернов\n",
    "- Следите за переобучением\n",
    "\n",
    "**Dropout:**\n",
    "- Типичные значения: 0.2-0.4\n",
    "- Увеличивайте при переобучении\n",
    "\n",
    "**Batch size:**\n",
    "- Меньшие значения (16-32): более шумное обучение, но может выйти из локальных минимумов\n",
    "- Большие значения (64-128): более стабильное обучение\n",
    "\n",
    "### 4.2 Когда использовать LSTM vs классические методы\n",
    "\n",
    "**LSTM предпочтительны когда:**\n",
    "- Большой объем данных (>1000 наблюдений)\n",
    "- Нелинейные зависимости\n",
    "- Множественные переменные (многомерные временные ряды)\n",
    "- Долгосрочные зависимости\n",
    "\n",
    "**Классические методы (ARIMA, Exponential Smoothing) предпочтительны когда:**\n",
    "- Малый объем данных (<500 наблюдений)\n",
    "- Необходима интерпретируемость\n",
    "- Простые линейные паттерны\n",
    "- Ограниченные вычислительные ресурсы\n",
    "\n",
    "### 4.3 Предотвращение переобучения\n",
    "\n",
    "1. **Dropout** - регуляризация\n",
    "2. **Early Stopping** - остановка при ухудшении валидационной ошибки\n",
    "3. **L1/L2 регуляризация** весов\n",
    "4. **Уменьшение сложности модели**\n",
    "5. **Аугментация данных** (добавление шума, сдвиги)\n",
    "6. **Cross-validation** для временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Задания для самостоятельной работы\n",
    "\n",
    "### Задание 1: Базовое (обязательное)\n",
    "\n",
    "Загрузите данные о пассажиропотоке авиакомпаний (Air Passengers dataset) и:\n",
    "1. Подготовьте данные для LSTM с window_size = 12\n",
    "2. Постройте и обучите LSTM модель\n",
    "3. Сравните качество с ARIMA моделью из предыдущих ноутбуков\n",
    "4. Визуализируйте результаты\n",
    "\n",
    "**Критерии оценки:**\n",
    "- Правильная подготовка данных\n",
    "- Работающая модель LSTM\n",
    "- Корректные метрики качества\n",
    "- Визуализация предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для решения задания 1\n",
    "# TODO: Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2: Продвинутое\n",
    "\n",
    "Реализуйте эксперимент по подбору гиперпараметров:\n",
    "1. Создайте функцию для обучения LSTM с различными параметрами\n",
    "2. Переберите различные комбинации:\n",
    "   - window_size: [20, 50, 100]\n",
    "   - units: [[32, 16], [64, 32], [128, 64]]\n",
    "   - dropout_rate: [0.1, 0.2, 0.3]\n",
    "3. Сохраните результаты в DataFrame\n",
    "4. Постройте визуализацию зависимости качества от гиперпараметров\n",
    "5. Выберите лучшую конфигурацию\n",
    "\n",
    "**Дополнительно:** Используйте Keras Tuner или Optuna для автоматического подбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для решения задания 2\n",
    "# TODO: Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3: Исследовательское\n",
    "\n",
    "Реализуйте многомерный прогноз временного ряда:\n",
    "1. Найдите или создайте датасет с несколькими связанными временными рядами (например, температура, влажность, давление)\n",
    "2. Постройте многомерную LSTM модель (multivariate forecasting)\n",
    "3. Сравните качество прогноза при использовании:\n",
    "   - Только целевой переменной (univariate)\n",
    "   - Всех переменных (multivariate)\n",
    "4. Проанализируйте, какие переменные наиболее важны\n",
    "5. Реализуйте механизм Attention и оцените его влияние\n",
    "\n",
    "**Критерии оценки:**\n",
    "- Корректная работа с многомерными данными\n",
    "- Сравнительный анализ\n",
    "- Интерпретация результатов\n",
    "- Реализация Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для решения задания 3\n",
    "# TODO: Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контрольные вопросы\n",
    "\n",
    "1. В чем заключается проблема затухающего градиента в простых RNN?\n",
    "2. Объясните назначение каждого из трех вентилей (gates) в LSTM.\n",
    "3. Чем GRU отличается от LSTM? Когда стоит использовать GRU вместо LSTM?\n",
    "4. Почему важна нормализация данных для нейронных сетей?\n",
    "5. Как работает механизм Attention в контексте временных рядов?\n",
    "6. Когда следует использовать Bidirectional LSTM?\n",
    "7. В чем разница между one-step и multi-step forecasting?\n",
    "8. Как предотвратить переобучение в LSTM моделях?\n",
    "9. Почему encoder-decoder архитектура эффективна для sequence-to-sequence задач?\n",
    "10. В каких ситуациях LSTM модели превосходят классические статистические методы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные материалы\n",
    "\n",
    "**Статьи:**\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
    "- Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation.\n",
    "- Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate.\n",
    "\n",
    "**Онлайн ресурсы:**\n",
    "- [Colah's Blog: Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Keras Time Series Forecasting Tutorial](https://keras.io/examples/timeseries/)\n",
    "- [TensorFlow Time Series Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n",
    "\n",
    "**Книги:**\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\n",
    "- Chollet, F. (2021). Deep Learning with Python, Second Edition. Manning Publications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}